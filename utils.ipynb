{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Copyright 2020 The HuggingFace Team. All rights reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "     http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\n",
    "Very heavily inspired by the official evaluation script for SQuAD version 2.0 which was modified by XLNet authors to\n",
    "update `find_best_threshold` scripts for SQuAD V2.0\n",
    "\n",
    "In addition to basic functionality, we also compute additional statistics and plot precision-recall curves if an\n",
    "additional na_prob.json file is provided. This file is expected to map question ID's to the model's predicted\n",
    "probability that a question is unanswerable.\n",
    "\n",
    "Modified version of \"squad_metrics.py\" adapated for CUAD."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import collections\n",
    "import json\n",
    "import math\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "\n",
    "from transformers.models.bert import BasicTokenizer\n",
    "from transformers.utils import logging              "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "logger = logging.get_logger(__name__)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def reformat_predicted_string(remaining_contract, predicted_string):\n",
    "    tokens = predicted_string.split()\n",
    "    assert len(tokens) > 0\n",
    "    end_idx = 0\n",
    "    for i, token in enumerate(tokens):\n",
    "        found = remaining_contract[end_idx:].find(token)\n",
    "        assert found != -1\n",
    "        end_idx += found\n",
    "        if i == 0:\n",
    "            start_idx = end_idx\n",
    "    end_idx += len(tokens[-1])\n",
    "    return remaining_contract[start_idx:end_idx]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def find_char_start_idx(contract, preceeding_tokens, predicted_string):\n",
    "    contract = \" \".join(contract.split())\n",
    "    assert predicted_string in contract\n",
    "    if contract.count(predicted_string) == 1:\n",
    "        return contract.find(predicted_string)\n",
    "\n",
    "    start_idx = 0\n",
    "    for token in preceeding_tokens:\n",
    "        found = contract[start_idx:].find(token)\n",
    "        assert found != -1\n",
    "        start_idx += found\n",
    "    start_idx += len(preceeding_tokens[-1])\n",
    "    remaining_str = contract[start_idx:]\n",
    "\n",
    "    remaining_idx = remaining_str.find(predicted_string)\n",
    "    assert remaining_idx != -1\n",
    "\n",
    "    return start_idx + remaining_idx"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def get_tokens(s):\n",
    "    if not s:\n",
    "        return []\n",
    "    return normalize_answer(s).split()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def compute_exact(a_gold, a_pred):\n",
    "    return int(normalize_answer(a_gold) == normalize_answer(a_pred))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def compute_f1(a_gold, a_pred):\n",
    "    gold_toks = get_tokens(a_gold)\n",
    "    pred_toks = get_tokens(a_pred)\n",
    "    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
    "    num_same = sum(common.values())\n",
    "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "        return int(gold_toks == pred_toks)\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(pred_toks)\n",
    "    recall = 1.0 * num_same / len(gold_toks)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def get_raw_scores(examples, preds):\n",
    "    \"\"\"\n",
    "    Computes the exact and f1 scores from the examples and the model predictions\n",
    "    \"\"\"\n",
    "    exact_scores = {}\n",
    "    f1_scores = {}\n",
    "\n",
    "    for example in examples:\n",
    "        qas_id = example.qas_id\n",
    "        gold_answers = [answer[\"text\"] for answer in example.answers if normalize_answer(answer[\"text\"])]\n",
    "\n",
    "        if not gold_answers:\n",
    "            # For unanswerable questions, only correct answer is empty string\n",
    "            gold_answers = [\"\"]\n",
    "\n",
    "        if qas_id not in preds:\n",
    "            print(\"Missing prediction for %s\" % qas_id)\n",
    "            continue\n",
    "\n",
    "        prediction = preds[qas_id]\n",
    "        exact_scores[qas_id] = max(compute_exact(a, prediction) for a in gold_answers)\n",
    "        f1_scores[qas_id] = max(compute_f1(a, prediction) for a in gold_answers)\n",
    "\n",
    "    return exact_scores, f1_scores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def apply_no_ans_threshold(scores, na_probs, qid_to_has_ans, na_prob_thresh):\n",
    "    new_scores = {}\n",
    "    for qid, s in scores.items():\n",
    "        pred_na = na_probs[qid] > na_prob_thresh\n",
    "        if pred_na:\n",
    "            new_scores[qid] = float(not qid_to_has_ans[qid])\n",
    "        else:\n",
    "            new_scores[qid] = s\n",
    "    return new_scores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def make_eval_dict(exact_scores, f1_scores, qid_list=None):\n",
    "    if not qid_list:\n",
    "        total = len(exact_scores)\n",
    "        return collections.OrderedDict(\n",
    "            [\n",
    "                (\"exact\", 100.0 * sum(exact_scores.values()) / total),\n",
    "                (\"f1\", 100.0 * sum(f1_scores.values()) / total),\n",
    "                (\"total\", total),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        total = len(qid_list)\n",
    "        return collections.OrderedDict(\n",
    "            [\n",
    "                (\"exact\", 100.0 * sum(exact_scores[k] for k in qid_list) / total),\n",
    "                (\"f1\", 100.0 * sum(f1_scores[k] for k in qid_list) / total),\n",
    "                (\"total\", total),\n",
    "            ]\n",
    "        )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def merge_eval(main_eval, new_eval, prefix):\n",
    "    for k in new_eval:\n",
    "        main_eval[\"%s_%s\" % (prefix, k)] = new_eval[k]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def find_best_thresh_v2(preds, scores, na_probs, qid_to_has_ans):\n",
    "    num_no_ans = sum(1 for k in qid_to_has_ans if not qid_to_has_ans[k])\n",
    "    cur_score = num_no_ans\n",
    "    best_score = cur_score\n",
    "    best_thresh = 0.0\n",
    "    qid_list = sorted(na_probs, key=lambda k: na_probs[k])\n",
    "    for i, qid in enumerate(qid_list):\n",
    "        if qid not in scores:\n",
    "            continue\n",
    "        if qid_to_has_ans[qid]:\n",
    "            diff = scores[qid]\n",
    "        else:\n",
    "            if preds[qid]:\n",
    "                diff = -1\n",
    "            else:\n",
    "                diff = 0\n",
    "        cur_score += diff\n",
    "        if cur_score > best_score:\n",
    "            best_score = cur_score\n",
    "            best_thresh = na_probs[qid]\n",
    "\n",
    "    has_ans_score, has_ans_cnt = 0, 0\n",
    "    for qid in qid_list:\n",
    "        if not qid_to_has_ans[qid]:\n",
    "            continue\n",
    "        has_ans_cnt += 1\n",
    "\n",
    "        if qid not in scores:\n",
    "            continue\n",
    "        has_ans_score += scores[qid]\n",
    "\n",
    "    return 100.0 * best_score / len(scores), best_thresh, 1.0 * has_ans_score / has_ans_cnt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def find_all_best_thresh_v2(main_eval, preds, exact_raw, f1_raw, na_probs, qid_to_has_ans):\n",
    "    best_exact, exact_thresh, has_ans_exact = find_best_thresh_v2(preds, exact_raw, na_probs, qid_to_has_ans)\n",
    "    best_f1, f1_thresh, has_ans_f1 = find_best_thresh_v2(preds, f1_raw, na_probs, qid_to_has_ans)\n",
    "    # NOTE: For CUAD, which is about finding needles in haystacks and for which different answers should be treated\n",
    "    # differently, these metrics don't make complete sense. We ignore them, but don't remove them for simplicity.\n",
    "    main_eval[\"best_exact\"] = best_exact\n",
    "    main_eval[\"best_exact_thresh\"] = exact_thresh\n",
    "    main_eval[\"best_f1\"] = best_f1\n",
    "    main_eval[\"best_f1_thresh\"] = f1_thresh\n",
    "    main_eval[\"has_ans_exact\"] = has_ans_exact\n",
    "    main_eval[\"has_ans_f1\"] = has_ans_f1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def find_best_thresh(preds, scores, na_probs, qid_to_has_ans):\n",
    "    num_no_ans = sum(1 for k in qid_to_has_ans if not qid_to_has_ans[k])\n",
    "    cur_score = num_no_ans\n",
    "    best_score = cur_score\n",
    "    best_thresh = 0.0\n",
    "    qid_list = sorted(na_probs, key=lambda k: na_probs[k])\n",
    "    for _, qid in enumerate(qid_list):\n",
    "        if qid not in scores:\n",
    "            continue\n",
    "        if qid_to_has_ans[qid]:\n",
    "            diff = scores[qid]\n",
    "        else:\n",
    "            if preds[qid]:\n",
    "                diff = -1\n",
    "            else:\n",
    "                diff = 0\n",
    "        cur_score += diff\n",
    "        if cur_score > best_score:\n",
    "            best_score = cur_score\n",
    "            best_thresh = na_probs[qid]\n",
    "    return 100.0 * best_score / len(scores), best_thresh"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "def find_all_best_thresh(main_eval, preds, exact_raw, f1_raw, na_probs, qid_to_has_ans):\n",
    "    best_exact, exact_thresh = find_best_thresh(preds, exact_raw, na_probs, qid_to_has_ans)\n",
    "    best_f1, f1_thresh = find_best_thresh(preds, f1_raw, na_probs, qid_to_has_ans)\n",
    "\n",
    "    main_eval[\"best_exact\"] = best_exact\n",
    "    main_eval[\"best_exact_thresh\"] = exact_thresh\n",
    "    main_eval[\"best_f1\"] = best_f1\n",
    "    main_eval[\"best_f1_thresh\"] = f1_thresh"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "def squad_evaluate(examples, preds, no_answer_probs=None, no_answer_probability_threshold=1.0):\n",
    "    qas_id_to_has_answer = {example.qas_id: bool(example.answers) for example in examples}\n",
    "    has_answer_qids = [qas_id for qas_id, has_answer in qas_id_to_has_answer.items() if has_answer]\n",
    "    no_answer_qids = [qas_id for qas_id, has_answer in qas_id_to_has_answer.items() if not has_answer]\n",
    "\n",
    "    if no_answer_probs is None:\n",
    "        no_answer_probs = {k: 0.0 for k in preds}\n",
    "\n",
    "    exact, f1 = get_raw_scores(examples, preds)\n",
    "\n",
    "    exact_threshold = apply_no_ans_threshold(\n",
    "        exact, no_answer_probs, qas_id_to_has_answer, no_answer_probability_threshold\n",
    "    )\n",
    "    f1_threshold = apply_no_ans_threshold(f1, no_answer_probs, qas_id_to_has_answer, no_answer_probability_threshold)\n",
    "\n",
    "    evaluation = make_eval_dict(exact_threshold, f1_threshold)\n",
    "\n",
    "    if has_answer_qids:\n",
    "        has_ans_eval = make_eval_dict(exact_threshold, f1_threshold, qid_list=has_answer_qids)\n",
    "        merge_eval(evaluation, has_ans_eval, \"HasAns\")\n",
    "\n",
    "    if no_answer_qids:\n",
    "        no_ans_eval = make_eval_dict(exact_threshold, f1_threshold, qid_list=no_answer_qids)\n",
    "        merge_eval(evaluation, no_ans_eval, \"NoAns\")\n",
    "\n",
    "    if no_answer_probs:\n",
    "        find_all_best_thresh(evaluation, preds, exact, f1, no_answer_probs, qas_id_to_has_answer)\n",
    "\n",
    "    return evaluation"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "def get_final_text(pred_text, orig_text, do_lower_case, verbose_logging=False):\n",
    "    \"\"\"Project the tokenized prediction back to the original text.\"\"\"\n",
    "\n",
    "    # When we created the data, we kept track of the alignment between original\n",
    "    # (whitespace tokenized) tokens and our WordPiece tokenized tokens. So\n",
    "    # now `orig_text` contains the span of our original text corresponding to the\n",
    "    # span that we predicted.\n",
    "    #\n",
    "    # However, `orig_text` may contain extra characters that we don't want in\n",
    "    # our prediction.\n",
    "    #\n",
    "    # For example, let's say:\n",
    "    #   pred_text = steve smith\n",
    "    #   orig_text = Steve Smith's\n",
    "    #\n",
    "    # We don't want to return `orig_text` because it contains the extra \"'s\".\n",
    "    #\n",
    "    # We don't want to return `pred_text` because it's already been normalized\n",
    "    # (the SQuAD eval script also does punctuation stripping/lower casing but\n",
    "    # our tokenizer does additional normalization like stripping accent\n",
    "    # characters).\n",
    "    #\n",
    "    # What we really want to return is \"Steve Smith\".\n",
    "    #\n",
    "    # Therefore, we have to apply a semi-complicated alignment heuristic between\n",
    "    # `pred_text` and `orig_text` to get a character-to-character alignment. This\n",
    "    # can fail in certain cases in which case we just return `orig_text`.\n",
    "\n",
    "    def _strip_spaces(text):\n",
    "        ns_chars = []\n",
    "        ns_to_s_map = collections.OrderedDict()\n",
    "        for (i, c) in enumerate(text):\n",
    "            if c == \" \":\n",
    "                continue\n",
    "            ns_to_s_map[len(ns_chars)] = i\n",
    "            ns_chars.append(c)\n",
    "        ns_text = \"\".join(ns_chars)\n",
    "        return (ns_text, ns_to_s_map)\n",
    "\n",
    "    # We first tokenize `orig_text`, strip whitespace from the result\n",
    "    # and `pred_text`, and check if they are the same length. If they are\n",
    "    # NOT the same length, the heuristic has failed. If they are the same\n",
    "    # length, we assume the characters are one-to-one aligned.\n",
    "    tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n",
    "\n",
    "    tok_text = \" \".join(tokenizer.tokenize(orig_text))\n",
    "\n",
    "    start_position = tok_text.find(pred_text)\n",
    "    if start_position == -1:\n",
    "        if verbose_logging:\n",
    "            logger.info(\"Unable to find text: '%s' in '%s'\" % (pred_text, orig_text))\n",
    "        return orig_text\n",
    "    end_position = start_position + len(pred_text) - 1\n",
    "\n",
    "    (orig_ns_text, orig_ns_to_s_map) = _strip_spaces(orig_text)\n",
    "    (tok_ns_text, tok_ns_to_s_map) = _strip_spaces(tok_text)\n",
    "\n",
    "    if len(orig_ns_text) != len(tok_ns_text):\n",
    "        if verbose_logging:\n",
    "            logger.info(\"Length not equal after stripping spaces: '%s' vs '%s'\", orig_ns_text, tok_ns_text)\n",
    "        return orig_text\n",
    "\n",
    "    # We then project the characters in `pred_text` back to `orig_text` using\n",
    "    # the character-to-character alignment.\n",
    "    tok_s_to_ns_map = {}\n",
    "    for (i, tok_index) in tok_ns_to_s_map.items():\n",
    "        tok_s_to_ns_map[tok_index] = i\n",
    "\n",
    "    orig_start_position = None\n",
    "    if start_position in tok_s_to_ns_map:\n",
    "        ns_start_position = tok_s_to_ns_map[start_position]\n",
    "        if ns_start_position in orig_ns_to_s_map:\n",
    "            orig_start_position = orig_ns_to_s_map[ns_start_position]\n",
    "\n",
    "    if orig_start_position is None:\n",
    "        if verbose_logging:\n",
    "            logger.info(\"Couldn't map start position\")\n",
    "        return orig_text\n",
    "\n",
    "    orig_end_position = None\n",
    "    if end_position in tok_s_to_ns_map:\n",
    "        ns_end_position = tok_s_to_ns_map[end_position]\n",
    "        if ns_end_position in orig_ns_to_s_map:\n",
    "            orig_end_position = orig_ns_to_s_map[ns_end_position]\n",
    "\n",
    "    if orig_end_position is None:\n",
    "        if verbose_logging:\n",
    "            logger.info(\"Couldn't map end position\")\n",
    "        return orig_text\n",
    "\n",
    "    output_text = orig_text[orig_start_position : (orig_end_position + 1)]\n",
    "    return output_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "def _get_best_indexes(logits, n_best_size):\n",
    "    \"\"\"Get the n-best logits from a list.\"\"\"\n",
    "    index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    best_indexes = []\n",
    "    for i in range(len(index_and_score)):\n",
    "        if i >= n_best_size:\n",
    "            break\n",
    "        best_indexes.append(index_and_score[i][0])\n",
    "    return best_indexes"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "def _compute_softmax(scores):\n",
    "    \"\"\"Compute softmax probability over raw logits.\"\"\"\n",
    "    if not scores:\n",
    "        return []\n",
    "\n",
    "    max_score = None\n",
    "    for score in scores:\n",
    "        if max_score is None or score > max_score:\n",
    "            max_score = score\n",
    "\n",
    "    exp_scores = []\n",
    "    total_sum = 0.0\n",
    "    for score in scores:\n",
    "        x = math.exp(score - max_score)\n",
    "        exp_scores.append(x)\n",
    "        total_sum += x\n",
    "\n",
    "    probs = []\n",
    "    for score in exp_scores:\n",
    "        probs.append(score / total_sum)\n",
    "    return probs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "def compute_predictions_logits(\n",
    "    json_input_dict,\n",
    "    all_examples,\n",
    "    all_features,\n",
    "    all_results,\n",
    "    n_best_size,\n",
    "    max_answer_length,\n",
    "    do_lower_case,\n",
    "    output_prediction_file,\n",
    "    output_nbest_file,\n",
    "    output_null_log_odds_file,\n",
    "    verbose_logging,\n",
    "    version_2_with_negative,\n",
    "    null_score_diff_threshold,\n",
    "    tokenizer,\n",
    "):\n",
    "    \"\"\"Write final predictions to the json file and log-odds of null if needed.\"\"\"\n",
    "    if output_prediction_file:\n",
    "        logger.info(f\"Writing predictions to: {output_prediction_file}\")\n",
    "    if output_nbest_file:\n",
    "        logger.info(f\"Writing nbest to: {output_nbest_file}\")\n",
    "    if output_null_log_odds_file and version_2_with_negative:\n",
    "        logger.info(f\"Writing null_log_odds to: {output_null_log_odds_file}\")\n",
    "\n",
    "    example_index_to_features = collections.defaultdict(list)\n",
    "    for feature in all_features:\n",
    "        example_index_to_features[feature.example_index].append(feature)\n",
    "\n",
    "    unique_id_to_result = {}\n",
    "    for result in all_results:\n",
    "        unique_id_to_result[result.unique_id] = result\n",
    "\n",
    "    _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "        \"PrelimPrediction\", [\"feature_index\", \"start_index\", \"end_index\", \"start_logit\", \"end_logit\"]\n",
    "    )\n",
    "\n",
    "    all_predictions = collections.OrderedDict()\n",
    "    all_nbest_json = collections.OrderedDict()\n",
    "    scores_diff_json = collections.OrderedDict()\n",
    "\n",
    "    contract_name_to_idx = {}\n",
    "    for idx in range(len(json_input_dict[\"data\"])):\n",
    "        contract_name_to_idx[json_input_dict[\"data\"][idx][\"title\"]] = idx\n",
    "\n",
    "    for (example_index, example) in enumerate(all_examples):\n",
    "        features = example_index_to_features[example_index]\n",
    "\n",
    "        contract_name = example.title\n",
    "        contract_index = contract_name_to_idx[contract_name]\n",
    "        paragraphs = json_input_dict[\"data\"][contract_index][\"paragraphs\"]\n",
    "        assert len(paragraphs) == 1\n",
    "\n",
    "        prelim_predictions = []\n",
    "        # keep track of the minimum score of null start+end of position 0\n",
    "        score_null = 1000000  # large and positive\n",
    "        min_null_feature_index = 0  # the paragraph slice with min null score\n",
    "        null_start_logit = 0  # the start logit at the slice with min null score\n",
    "        null_end_logit = 0  # the end logit at the slice with min null score\n",
    "        for (feature_index, feature) in enumerate(features):\n",
    "            result = unique_id_to_result[feature.unique_id]\n",
    "            start_indexes = _get_best_indexes(result.start_logits, n_best_size)\n",
    "            end_indexes = _get_best_indexes(result.end_logits, n_best_size)\n",
    "            # if we could have irrelevant answers, get the min score of irrelevant\n",
    "            if version_2_with_negative:\n",
    "                feature_null_score = result.start_logits[0] + result.end_logits[0]\n",
    "                if feature_null_score < score_null:\n",
    "                    score_null = feature_null_score\n",
    "                    min_null_feature_index = feature_index\n",
    "                    null_start_logit = result.start_logits[0]\n",
    "                    null_end_logit = result.end_logits[0]\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # We could hypothetically create invalid predictions, e.g., predict\n",
    "                    # that the start of the span is in the question. We throw out all\n",
    "                    # invalid predictions.\n",
    "                    if start_index >= len(feature.tokens):\n",
    "                        continue\n",
    "                    if end_index >= len(feature.tokens):\n",
    "                        continue\n",
    "                    if start_index not in feature.token_to_orig_map:\n",
    "                        continue\n",
    "                    if end_index not in feature.token_to_orig_map:\n",
    "                        continue\n",
    "                    if not feature.token_is_max_context.get(start_index, False):\n",
    "                        continue\n",
    "                    if end_index < start_index:\n",
    "                        continue\n",
    "                    length = end_index - start_index + 1\n",
    "                    if length > max_answer_length:\n",
    "                        continue\n",
    "                    prelim_predictions.append(\n",
    "                        _PrelimPrediction(\n",
    "                            feature_index=feature_index,\n",
    "                            start_index=start_index,\n",
    "                            end_index=end_index,\n",
    "                            start_logit=result.start_logits[start_index],\n",
    "                            end_logit=result.end_logits[end_index],\n",
    "                        )\n",
    "                    )\n",
    "        if version_2_with_negative:\n",
    "            prelim_predictions.append(\n",
    "                _PrelimPrediction(\n",
    "                    feature_index=min_null_feature_index,\n",
    "                    start_index=0,\n",
    "                    end_index=0,\n",
    "                    start_logit=null_start_logit,\n",
    "                    end_logit=null_end_logit,\n",
    "                )\n",
    "            )\n",
    "        prelim_predictions = sorted(prelim_predictions, key=lambda x: (x.start_logit + x.end_logit), reverse=True)\n",
    "\n",
    "        _NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "            \"NbestPrediction\", [\"text\", \"start_logit\", \"end_logit\"]\n",
    "        )\n",
    "\n",
    "        seen_predictions = {}\n",
    "        nbest = []\n",
    "        start_indexes = []\n",
    "        end_indexes = []\n",
    "        for pred in prelim_predictions:\n",
    "            if len(nbest) >= n_best_size:\n",
    "                break\n",
    "            feature = features[pred.feature_index]\n",
    "            if pred.start_index > 0:  # this is a non-null prediction\n",
    "                tok_tokens = feature.tokens[pred.start_index : (pred.end_index + 1)]\n",
    "                orig_doc_start = feature.token_to_orig_map[pred.start_index]\n",
    "                orig_doc_end = feature.token_to_orig_map[pred.end_index]\n",
    "                orig_tokens = example.doc_tokens[orig_doc_start : (orig_doc_end + 1)]\n",
    "\n",
    "                tok_text = tokenizer.convert_tokens_to_string(tok_tokens)\n",
    "\n",
    "                # Clean whitespace\n",
    "                tok_text = tok_text.strip()\n",
    "                tok_text = \" \".join(tok_text.split())\n",
    "                orig_text = \" \".join(orig_tokens)\n",
    "\n",
    "                final_text = get_final_text(tok_text, orig_text, do_lower_case, verbose_logging)\n",
    "\n",
    "                if final_text in seen_predictions:\n",
    "                    continue\n",
    "\n",
    "                seen_predictions[final_text] = True\n",
    "\n",
    "                start_indexes.append(orig_doc_start)\n",
    "                end_indexes.append(orig_doc_end)\n",
    "            else:\n",
    "                final_text = \"\"\n",
    "                seen_predictions[final_text] = True\n",
    "\n",
    "                start_indexes.append(-1)\n",
    "                end_indexes.append(-1)\n",
    "\n",
    "            nbest.append(_NbestPrediction(text=final_text, start_logit=pred.start_logit, end_logit=pred.end_logit))\n",
    "\n",
    "        # if we didn't include the empty option in the n-best, include it\n",
    "        if version_2_with_negative:\n",
    "            if \"\" not in seen_predictions:\n",
    "                nbest.append(_NbestPrediction(text=\"\", start_logit=null_start_logit, end_logit=null_end_logit))\n",
    "                start_indexes.append(-1)\n",
    "                end_indexes.append(-1)\n",
    "\n",
    "            # In very rare edge cases we could only have single null prediction.\n",
    "            # So we just create a nonce prediction in this case to avoid failure.\n",
    "            if len(nbest) == 1:\n",
    "                nbest.insert(0, _NbestPrediction(text=\"empty\", start_logit=0.0, end_logit=0.0))\n",
    "                start_indexes.append(-1)\n",
    "                end_indexes.append(-1)\n",
    "\n",
    "        # In very rare edge cases we could have no valid predictions. So we\n",
    "        # just create a nonce prediction in this case to avoid failure.\n",
    "        if not nbest:\n",
    "            nbest.append(_NbestPrediction(text=\"empty\", start_logit=0.0, end_logit=0.0))\n",
    "            start_indexes.append(-1)\n",
    "            end_indexes.append(-1)\n",
    "\n",
    "        assert len(nbest) >= 1, \"No valid predictions\"\n",
    "        assert len(nbest) == len(start_indexes), \"nbest length: {}, start_indexes length: {}\".format(len(nbest), len(start_indexes))\n",
    "\n",
    "        total_scores = []\n",
    "        best_non_null_entry = None\n",
    "        for entry in nbest:\n",
    "            total_scores.append(entry.start_logit + entry.end_logit)\n",
    "            if not best_non_null_entry:\n",
    "                if entry.text:\n",
    "                    best_non_null_entry = entry\n",
    "\n",
    "        probs = _compute_softmax(total_scores)\n",
    "\n",
    "        nbest_json = []\n",
    "        for (i, entry) in enumerate(nbest):\n",
    "            output = collections.OrderedDict()\n",
    "            output[\"text\"] = entry.text\n",
    "            output[\"probability\"] = probs[i]\n",
    "            output[\"start_logit\"] = entry.start_logit\n",
    "            output[\"end_logit\"] = entry.end_logit\n",
    "            output[\"token_doc_start\"] = start_indexes[i]\n",
    "            output[\"token_doc_end\"] = end_indexes[i]\n",
    "            nbest_json.append(output)\n",
    "\n",
    "        assert len(nbest_json) >= 1, \"No valid predictions\"\n",
    "\n",
    "        if not version_2_with_negative:\n",
    "            all_predictions[example.qas_id] = nbest_json[0][\"text\"]\n",
    "        else:\n",
    "            # predict \"\" iff the null score - the score of best non-null > threshold\n",
    "            score_diff = score_null - best_non_null_entry.start_logit - (best_non_null_entry.end_logit)\n",
    "            scores_diff_json[example.qas_id] = score_diff\n",
    "            if score_diff > null_score_diff_threshold:\n",
    "                all_predictions[example.qas_id] = \"\"\n",
    "            else:\n",
    "                all_predictions[example.qas_id] = best_non_null_entry.text\n",
    "        all_nbest_json[example.qas_id] = nbest_json\n",
    "\n",
    "    if output_prediction_file:\n",
    "        with open(output_prediction_file, \"w\") as writer:\n",
    "            writer.write(json.dumps(all_predictions, indent=4) + \"\\n\")\n",
    "\n",
    "    if output_nbest_file:\n",
    "        with open(output_nbest_file, \"w\") as writer:\n",
    "            writer.write(json.dumps(all_nbest_json, indent=4) + \"\\n\")\n",
    "\n",
    "    if output_null_log_odds_file and version_2_with_negative:\n",
    "        with open(output_null_log_odds_file, \"w\") as writer:\n",
    "            writer.write(json.dumps(scores_diff_json, indent=4) + \"\\n\")\n",
    "\n",
    "    return all_predictions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}